{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11625475,"sourceType":"datasetVersion","datasetId":7293458},{"sourceId":12041030,"sourceType":"datasetVersion","datasetId":7576939}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --quiet pandas numpy scikit-learn torch transformers rdkit-pypi fair-esm optuna matplotlib seaborn scipy","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\n01_Data_Preprocessing_DTI.py\n\nPreprocesses BindingDB and AID datasets for DTI modeling with PfDHODH as the fixed target.\n- Standardizes SMILES and protein sequence\n- Handles NaN/inf robustly (removes or fills as appropriate)\n- Outputs: processed_bindingdb_dti.csv, processed_aid_merged_dti.csv, processed_merged_all_dti.csv\n\nAuthor: [Salau Shina]\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom rdkit import Chem, RDLogger\nfrom rdkit.Chem import SaltRemover\n\nRDLogger.DisableLog('rdApp.*')\n\n# --- Configuration ---\nBINDINGDB_PATH = '/kaggle/input/bindingdb/BindingDB_All.tsv'\nAID_PATHS = [\n    '/kaggle/input/punched-aid/AID_504832.csv',\n    '/kaggle/input/punched-aid/AID_504834.csv'\n]\nTARGET_COLUMN_BINDINGDB = 'Kd (nM)'\nTARGET_COLUMN_AID = 'Fit_LogAC50'\nSMILES_COLUMN_BINDINGDB = 'Ligand SMILES'\nSMILES_COLUMN_AIDs = 'PUBCHEM_EXT_DATASOURCE_SMILES'\nCHUNK_SIZE = 100_000\n\n# Fixed protein target: PfDHODH (Plasmodium falciparum Dihydroorotate Dehydrogenase)\nPF_DHODH_SEQUENCE = \"MISKLKPQFMFLPKKHILSYCRKDVLNLFEQKFYYTSKRKESNNMKNESLLRLINYNRYYNKIDSNNYYNGGKILSNDRQYIYSPLCEYKKKINDISSYVSVPFKINIRNLGTSNFVNNKKDVLDNDYIYENIKKEKSKHKKIIFLLFVSLFGLYGFFESYNPEFFLYDIFLKFCLKYIDGEICHDLFLLLGKYNILPYDTSNDSIYACTNIKHLDFINPFGVAAGFDKNGVCIDSILKLGFSFIEIGTITPRGQTGNAKPRIFRDVESRSIINSCGFNNMGCDKVTENLILFRKRQEEDKLLSKHIVGVSIGKNKDTVNIVDDLKYCINKIGRYADYIAINVSSPNTPGLRDNQEAGKLKNIILSVKEEIDNLEKNNIMNDESTYNEDNKIVEKKNNFNKNNSHMMKDAKDNFLWFNTTKKKPLVFVKLAPDLNQEQKKEIADVLLETNIDGMIISNTTTQINDIKSFENKKGGVSGAKLKDISTKFICEMYNYTNKQIPIIASGGIFSGLDALEKIEAGASVCQLYSCLVFNGMKSAVQIKRELNHLLYQRGYYNLKEAIGRKHSKS\"\n\n# --- Helper Functions ---\nfrom rdkit.Chem.Scaffolds import MurckoScaffold\n\ndef standardize_smiles(smiles, remover):\n    \"\"\"Standardize SMILES by removing salts and canonicalizing.\"\"\"\n    if pd.isna(smiles):\n        return None\n    try:\n        mol = Chem.MolFromSmiles(smiles)\n        if mol:\n            mol = remover.StripMol(mol)\n            return Chem.MolToSmiles(mol, canonical=True)\n        return None\n    except:\n        return None\n\ndef generate_scaffold(smiles):\n    \"\"\"Generate Bemis-Murcko scaffold from SMILES.\"\"\"\n    mol = Chem.MolFromSmiles(smiles)\n    if mol is None:\n        return None\n    return MurckoScaffold.MurckoSmiles(mol=mol, includeChirality=False)\n\ndef convert_kd_to_logkd(kd_str):\n    if pd.isna(kd_str):\n        return np.nan\n    s_val = str(kd_str).strip()\n    try:\n        if s_val.startswith('<'):\n            val = float(s_val[1:]) * 0.5\n        elif s_val.startswith('>'):\n            val = float(s_val[1:]) * 2.0\n        else:\n            val = float(s_val)\n        return -np.log10(val * 1e-9)\n    except ValueError:\n        return np.nan\n\n# --- Scaffold-based K-fold split for DTI (reproducible, correct indices) ---\ndef scaffold_split_kfold(df, n_folds=5, seed=42):\n    import numpy as np\n    np.random.seed(seed)\n    from collections import defaultdict\n    df_temp = df.dropna(subset=['SMILES']).reset_index()\n    scaffolds = defaultdict(list)\n    for idx, smi in enumerate(df_temp['SMILES']):\n        scaffold = generate_scaffold(smi)\n        scaffolds[scaffold].append(df_temp.index[idx])\n    scaffold_sets = sorted(scaffolds.values(), key=lambda x: len(x), reverse=True)\n    folds = [[] for _ in range(n_folds)]\n    for i, sset in enumerate(scaffold_sets):\n        folds[i % n_folds].extend(sset)\n    all_indices = df.index.tolist()\n    for k in range(n_folds):\n        val_idx = folds[k]\n        train_idx = [i for i in all_indices if i not in val_idx]\n        yield train_idx, val_idx\n\ndef process_bindingdb_in_chunks(path, remover):\n    processed_chunks = []\n    try:\n        for chunk in pd.read_csv(path, sep='\\t', chunksize=CHUNK_SIZE):\n            if SMILES_COLUMN_BINDINGDB not in chunk or TARGET_COLUMN_BINDINGDB not in chunk:\n                continue\n            chunk = chunk[[SMILES_COLUMN_BINDINGDB, TARGET_COLUMN_BINDINGDB]].copy()\n            chunk.rename(columns={SMILES_COLUMN_BINDINGDB: 'SMILES', TARGET_COLUMN_BINDINGDB: 'Kd_nM'}, inplace=True)\n            chunk['SMILES'] = chunk['SMILES'].apply(lambda x: standardize_smiles(x, remover))\n            chunk.dropna(subset=['SMILES'], inplace=True)\n            chunk['log_kd'] = chunk['Kd_nM'].apply(convert_kd_to_logkd)\n            chunk.dropna(subset=['log_kd'], inplace=True)\n            chunk.drop(columns=['Kd_nM'], inplace=True)\n            # Add protein sequence column\n            chunk['Protein_Sequence'] = PF_DHODH_SEQUENCE\n            processed_chunks.append(chunk)\n        if processed_chunks:\n            df = pd.concat(processed_chunks, ignore_index=True)\n            df.drop_duplicates(subset=['SMILES', 'Protein_Sequence'], inplace=True)\n            print(f\"Processed BindingDB DTI: {len(df)} entries.\")\n            return df\n        else:\n            print(\"No valid BindingDB DTI data found.\")\n            return pd.DataFrame()\n    except FileNotFoundError:\n        print(f\"Error: BindingDB file not found at {path}\")\n        return pd.DataFrame()\n\ndef process_aid_in_chunks(paths, remover):\n    processed_chunks = []\n    for path in paths:\n        try:\n            for chunk in pd.read_csv(path, chunksize=CHUNK_SIZE):\n                if SMILES_COLUMN_AIDs not in chunk or TARGET_COLUMN_AID not in chunk:\n                    continue\n                chunk = chunk[[SMILES_COLUMN_AIDs, TARGET_COLUMN_AID]].copy()\n                chunk.rename(columns={SMILES_COLUMN_AIDs: 'SMILES', TARGET_COLUMN_AID: 'pAC50_raw'}, inplace=True)\n                chunk['SMILES'] = chunk['SMILES'].apply(lambda x: standardize_smiles(x, remover))\n                chunk.dropna(subset=['SMILES'], inplace=True)\n                chunk['pAC50'] = pd.to_numeric(chunk['pAC50_raw'], errors='coerce')\n                chunk.dropna(subset=['pAC50'], inplace=True)\n                chunk.drop(columns=['pAC50_raw'], inplace=True)\n                # Add protein sequence column\n                chunk['Protein_Sequence'] = PF_DHODH_SEQUENCE\n                processed_chunks.append(chunk)\n            print(f\"Processed AID DTI dataset from {path}\")\n        except FileNotFoundError:\n            print(f\"Error: AID file not found at {path}\")\n        except Exception as e:\n            print(f\"Error loading {path}: {e}\")\n    if processed_chunks:\n        df = pd.concat(processed_chunks, ignore_index=True)\n        df.drop_duplicates(subset=['SMILES', 'Protein_Sequence'], inplace=True)\n        print(f\"Processed merged AID DTI: {len(df)} entries.\")\n        return df\n    else:\n        print(\"No valid AID DTI data found.\")\n        return pd.DataFrame()\n\ndef main():\n    remover = SaltRemover.SaltRemover()\n    df_bindingdb = process_bindingdb_in_chunks(BINDINGDB_PATH, remover)\n    df_aid_merged = process_aid_in_chunks(AID_PATHS, remover)\n\n    # Clean processed_bindingdb DTI\n    if not df_bindingdb.empty:\n        df_bindingdb = df_bindingdb.replace([np.inf, -np.inf], np.nan)\n        df_bindingdb = df_bindingdb.dropna(subset=['SMILES', 'log_kd', 'Protein_Sequence'])\n        df_bindingdb.to_csv('processed_bindingdb_dti.csv', index=False)\n        print(\"Processed BindingDB DTI saved to 'processed_bindingdb_dti.csv'\")\n    # Clean processed_aid_merged DTI\n    if not df_aid_merged.empty:\n        df_aid_merged = df_aid_merged.replace([np.inf, -np.inf], np.nan)\n        df_aid_merged = df_aid_merged.dropna(subset=['SMILES', 'pAC50', 'Protein_Sequence'])\n        df_aid_merged.to_csv('processed_aid_merged_dti.csv', index=False)\n        print(\"Processed AID merged DTI data saved to 'processed_aid_merged_dti.csv'\")\n\n    # Multi-task learning: Merge for cross-validation and fine-tuning\n    if not df_bindingdb.empty or not df_aid_merged.empty:\n        bindingdb_for_merge = df_bindingdb[['SMILES', 'Protein_Sequence', 'log_kd']].copy()\n        bindingdb_for_merge['pAC50'] = np.nan\n        aid_for_merge = df_aid_merged[['SMILES', 'Protein_Sequence', 'pAC50']].copy()\n        aid_for_merge['log_kd'] = np.nan\n        bindingdb_for_merge = bindingdb_for_merge[['SMILES', 'Protein_Sequence', 'log_kd', 'pAC50']]\n        aid_for_merge = aid_for_merge[['SMILES', 'Protein_Sequence', 'log_kd', 'pAC50']]\n        df_merged_all = pd.concat([bindingdb_for_merge, aid_for_merge], ignore_index=True)\n        # Remove rows where both log_kd and pAC50 are missing or inf\n        df_merged_all = df_merged_all.replace([np.inf, -np.inf], np.nan)\n        df_merged_all = df_merged_all[(df_merged_all['log_kd'].notna()) | (df_merged_all['pAC50'].notna())]\n        # Do NOT fill NaN targets with 0.0; keep NaN for masking in multitask learning\n        df_merged_all.drop_duplicates(subset=['SMILES', 'Protein_Sequence'], inplace=True)\n        df_merged_all.to_csv('processed_merged_all_dti.csv', index=False)\n        print(\"Processed and merged BindingDB and AID DTI data saved to 'processed_merged_all_dti.csv'\")\n\n    print(\"\\n--- Preprocessing DTI Summary ---\")\n    if not df_bindingdb.empty:\n        print(f\"BindingDB DTI (log_kd): {len(df_bindingdb)} entries\")\n    if not df_aid_merged.empty:\n        print(f\"AID DTI (pAC50): {len(df_aid_merged)} entries\")\n    if 'df_merged_all' in locals():\n        print(f\"Merged All DTI (log_kd & pAC50): {len(df_merged_all)} entries\")\n        print(f\"Merged All DTI (only log_kd available): {df_merged_all['log_kd'].notna().sum()} entries\")\n        print(f\"Merged All DTI (only pAC50 available): {df_merged_all['pAC50'].notna().sum()} entries\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
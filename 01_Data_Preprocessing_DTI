{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6458c82",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-05T12:11:18.420606Z",
     "iopub.status.busy": "2025-07-05T12:11:18.420052Z",
     "iopub.status.idle": "2025-07-05T12:12:33.029734Z",
     "shell.execute_reply": "2025-07-05T12:12:33.028759Z"
    },
    "papermill": {
     "duration": 74.613883,
     "end_time": "2025-07-05T12:12:33.031389",
     "exception": false,
     "start_time": "2025-07-05T12:11:18.417506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install --quiet pandas numpy scikit-learn torch transformers rdkit-pypi fair-esm optuna matplotlib seaborn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ce208da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T12:12:33.064517Z",
     "iopub.status.busy": "2025-07-05T12:12:33.064256Z",
     "iopub.status.idle": "2025-07-05T13:06:37.168838Z",
     "shell.execute_reply": "2025-07-05T13:06:37.167981Z"
    },
    "papermill": {
     "duration": 3244.122088,
     "end_time": "2025-07-05T13:06:37.170160",
     "exception": false,
     "start_time": "2025-07-05T12:12:33.048072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/2404192709.py:95: DtypeWarning: Columns (7,8,9,10,11,15,17,21,28,33,35,40,41,42,43,45,46,47,48,50,51,52,53,54,55,57,58,59,62,63,64,65,66,67,69,70,71,72,74,75,76,77,78,79) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, sep='\\t', chunksize=CHUNK_SIZE):\n",
      "/tmp/ipykernel_19/2404192709.py:95: DtypeWarning: Columns (8,9,10,11,13,15,17,18,20,21,27,28,33,35,39,43,45,46,47,48,50,51,52,53,54,55,62,64,65,66,74,75,76,77,78,79) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, sep='\\t', chunksize=CHUNK_SIZE):\n",
      "/tmp/ipykernel_19/2404192709.py:95: DtypeWarning: Columns (8,9,10,11,15,17,21,27,28,32,33,35,36,39,45,46,47,48,50,51,52,53,54,55,57,58,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, sep='\\t', chunksize=CHUNK_SIZE):\n",
      "/tmp/ipykernel_19/2404192709.py:95: DtypeWarning: Columns (7,8,9,10,11,12,13,15,17,21,27,28,32,33,35,36,45,46,47,48,50,51,52,53,54,55,57,58,59,62,63,64,65,66,67,69,70,71,74,75,76,77,78,79,86,87,88,89,90,91) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, sep='\\t', chunksize=CHUNK_SIZE):\n",
      "/tmp/ipykernel_19/2404192709.py:95: DtypeWarning: Columns (7,8,9,10,11,15,17,27,28,32,33,35,36,39,43,45,46,47,48,50,51,52,53,54,55,57,58,59,60,62,63,64,65,66,67,74,75,76,77,78,79,86,87,88,89,90,91,98,99,100,101,102,103,110,112,113,114,115,122,124,125,126,127,134,135,136,137,138,139) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, sep='\\t', chunksize=CHUNK_SIZE):\n",
      "/tmp/ipykernel_19/2404192709.py:95: DtypeWarning: Columns (7,8,9,10,11,15,27,28,32,33,35,36,45,46,47,48,50,51,52,53,54,55,62,63,64,65,66,67) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, sep='\\t', chunksize=CHUNK_SIZE):\n",
      "/tmp/ipykernel_19/2404192709.py:95: DtypeWarning: Columns (7,8,9,10,11,27,28,32,33,35,36,43,45,46,47,48,50,51,52,53,54,55,62,63,64,65,66,67,74,75,76,77,78,79,86,87,88,89,90,91) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, sep='\\t', chunksize=CHUNK_SIZE):\n",
      "/tmp/ipykernel_19/2404192709.py:95: DtypeWarning: Columns (7,8,9,10,11,27,28,32,33,35,36,43,45,46,47,48,50,51,52,53,54,55,62,63,64,65,66,67) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, sep='\\t', chunksize=CHUNK_SIZE):\n",
      "/tmp/ipykernel_19/2404192709.py:95: DtypeWarning: Columns (7,8,9,10,11,17,27,28,32,33,35,36,43,45,46,47,48,50,51,52,53,54,55,62,63,64,65,66,67,74,75,76,77,78,79) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, sep='\\t', chunksize=CHUNK_SIZE):\n",
      "/tmp/ipykernel_19/2404192709.py:95: DtypeWarning: Columns (7,8,9,10,11,17,27,28,32,33,35,36,45,46,47,48,50,51,52,53,54,55,62,63,64,65,66,67,74,75,76,77,78,79,86,87,88,89,90,91) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, sep='\\t', chunksize=CHUNK_SIZE):\n",
      "/tmp/ipykernel_19/2404192709.py:95: DtypeWarning: Columns (7,8,9,10,11,17,21,27,28,32,33,35,36,45,46,47,48,50,51,52,53,54,55,57,58,59,62,63,64,65,66,67,74,75,76,77,78,79,86,87,88,89,90,91) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, sep='\\t', chunksize=CHUNK_SIZE):\n",
      "/tmp/ipykernel_19/2404192709.py:68: RuntimeWarning: divide by zero encountered in log10\n",
      "  return -np.log10(val * 1e-9)\n",
      "/tmp/ipykernel_19/2404192709.py:95: DtypeWarning: Columns (7,8,9,10,11,17,20,27,28,32,33,35,36,45,46,47,48,50,51,52,53,54,55,62,63,64,65,66,67,74,75,76,77,78,79,86,87,88,89,90,91) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, sep='\\t', chunksize=CHUNK_SIZE):\n",
      "/tmp/ipykernel_19/2404192709.py:68: RuntimeWarning: divide by zero encountered in log10\n",
      "  return -np.log10(val * 1e-9)\n",
      "/tmp/ipykernel_19/2404192709.py:95: DtypeWarning: Columns (7,8,9,10,11,17,20,27,28,32,33,35,36,40,41,42,43,45,46,47,48,50,51,52,53,54,55,59,62,63,64,65,66,67) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, sep='\\t', chunksize=CHUNK_SIZE):\n",
      "/tmp/ipykernel_19/2404192709.py:68: RuntimeWarning: divide by zero encountered in log10\n",
      "  return -np.log10(val * 1e-9)\n",
      "/tmp/ipykernel_19/2404192709.py:95: DtypeWarning: Columns (7,8,9,10,11,17,20,21,27,28,32,33,35,36,43,45,46,47,48,50,51,52,53,54,55,62,63,64,65,66,67) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, sep='\\t', chunksize=CHUNK_SIZE):\n",
      "/tmp/ipykernel_19/2404192709.py:95: DtypeWarning: Columns (2,3,8,9,10,11,13,15,17,20,21,22,23,27,28,32,33,35,36,39,45,46,47,48,50,51,52,53,54,55,57,58,59,62,63,64,65,66,67,74,75,76,77,78,79,86,87,88,89,90,91,98,99,100,101,102,103,110,111,112,113,114,115,122,123,124,125,126,127,134,135,136,137,138,139,146,148,149,150,151,158,159,160,161,162,163,170,172,173,174,182,183,184,185,186,194,196,197,198,199) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, sep='\\t', chunksize=CHUNK_SIZE):\n",
      "/tmp/ipykernel_19/2404192709.py:68: RuntimeWarning: divide by zero encountered in log10\n",
      "  return -np.log10(val * 1e-9)\n",
      "/tmp/ipykernel_19/2404192709.py:95: DtypeWarning: Columns (8,10,11,15,20,45,46,47,48,50,51,52,53,54,55,57,58,59,62,63,64,65,66,67,74,75,76,77,78,79,86,87,88,89,90,91,98,99,100,101,102,103,110,111,112,113,114,115,122,123,124,125,126,127,134,135,136,137,138,139,146,147,148,149,150,151,158,159,160,161,162,163,170,171,172,173,174,175,182,183,184,185,186,187,194,196,197,198,199,206,207,208,209,210,211,218,220,221,222,230,231,232,233,234,235,242,243,244,245,246,247,254,255,256,257,258,259) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, sep='\\t', chunksize=CHUNK_SIZE):\n",
      "/tmp/ipykernel_19/2404192709.py:95: DtypeWarning: Columns (8,10,11,13,45,46,47,48,50,51,52,53,54,55,57,58,59,62,63,64,65,66,67,69,70,71,74,75,76,77,78,79,86,87,88,89,90,91,98,99,100,101,102,103,110,111,112,113,114,115,122,123,124,125,126,127,134,135,136,137,138,139,146,148,149,150,151,158,159,160,161,162,163,170,172,173,174,175) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, sep='\\t', chunksize=CHUNK_SIZE):\n",
      "/tmp/ipykernel_19/2404192709.py:68: RuntimeWarning: divide by zero encountered in log10\n",
      "  return -np.log10(val * 1e-9)\n",
      "/tmp/ipykernel_19/2404192709.py:95: DtypeWarning: Columns (8,10,11,15,45,46,47,48,50,51,52,53,54,55,57,58,59,62,63,64,65,66,67,69,70,71,74,75,76,77,78,79,86,87,88,89,90,91,93,94,95,98,99,100,101,102,103,110,112,113,114,115,117,118,119,122,123,124,125,126,127,134,135,136,137,138,139,146,147,148,149,150,151,158,160,161,162,163) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, sep='\\t', chunksize=CHUNK_SIZE):\n",
      "/tmp/ipykernel_19/2404192709.py:95: DtypeWarning: Columns (8,10,11,15,20,45,46,47,48,50,51,52,53,54,55,57,58,59,62,63,64,65,66,67,74,75,76,77,78,79,86,87,88,89,90,91,98,99,100,101,102,103,110,112,113,114,115,122,123,124,125,126,127,134,135,136,137,138,139,146,148,149,150,151,158,159,160,161,162,163) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, sep='\\t', chunksize=CHUNK_SIZE):\n",
      "/tmp/ipykernel_19/2404192709.py:95: DtypeWarning: Columns (8,10,11,12,15,20,45,46,47,48,50,51,52,53,54,55,57,58,59,62,63,64,65,66,67,74,75,76,77,78,79,86,87,88,89,90,91,98,99,100,101,102,103,110,111,112,113,114,115,122,123,124,125,126,127,134,135,136,137,138,139,146,147,148,149,150,151,158,159,160,161,162,163,170,171,172,173,174,175,182,183,184,185,186,194,196,197,198,199,206,208,209,210,211,218,220,221,222,223,230,232,233,234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, sep='\\t', chunksize=CHUNK_SIZE):\n",
      "/tmp/ipykernel_19/2404192709.py:95: DtypeWarning: Columns (8,10,11,15,45,46,47,48,50,51,52,53,54,55,57,58,59,62,63,64,65,66,67,74,75,76,77,78,79,86,87,88,89,90,91,98,99,100,101,102,103,105,106,107,110,111,112,113,114,115,122,123,124,125,126,127,129,130,131,134,135,136,137,138,139,146,147,148,149,150,151,158,159,160,161,162,163,170,171,172,173,174,175,182,183,184,185,186,194,196,197,198,199,206,208,209,210,211,218,220,221,222,223,230,232,233,234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, sep='\\t', chunksize=CHUNK_SIZE):\n",
      "/tmp/ipykernel_19/2404192709.py:95: DtypeWarning: Columns (8,9,10,11,20,35,45,46,47,48,50,51,52,53,54,55,62,63,64,65,66,67,74,75,76,77,78,79,86,87,88,89,90,91,98,100,101,102,103,110,112,113,114,122,124,125,126,127,134,135,136,137,138,146,148,149,150,151,158,160,161,162,170,172,173,174,182,183,184,185,186,194,196,197,198,199) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, sep='\\t', chunksize=CHUNK_SIZE):\n",
      "/tmp/ipykernel_19/2404192709.py:95: DtypeWarning: Columns (8,10,11,45,46,47,48,50,51,52,53,54,55,57,58,59,62,63,64,65,66,67,74,75,76,77,78,79,86,87,88,89,90,91,98,99,100,101,102,103,110,112,113,114,115,122,123,124,125,126,127,134,136,137,138,139,146,148,149,150,151,158,159,160,161,162,163) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, sep='\\t', chunksize=CHUNK_SIZE):\n",
      "/tmp/ipykernel_19/2404192709.py:95: DtypeWarning: Columns (8,10,11,15,35,45,46,47,48,50,51,52,53,54,55,62,63,64,65,66,67,74,76,77,78,79,86,87,88,89,90,91,98,99,100,101,102,103) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, sep='\\t', chunksize=CHUNK_SIZE):\n",
      "/tmp/ipykernel_19/2404192709.py:95: DtypeWarning: Columns (8,10,11,15,20,45,46,47,48,50,51,52,53,54,55,57,58,59,62,63,64,65,66,67,74,75,76,77,78,79,86,87,88,89,90,91,98,99,100,101,102,103,110,111,112,113,114,115,122,123,124,125,126,127,134,135,136,137,138,139,146,147,148,149,150,151,158,159,160,161,162,163,170,171,172,173,174,175,182,183,184,185,186,187,194,196,197,198,199,206,207,208,209,210,211,218,220,221,222,230,231,232,233,234,235,242,243,244,245,246,247,254,255,256,257,258,259) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, sep='\\t', chunksize=CHUNK_SIZE):\n",
      "/tmp/ipykernel_19/2404192709.py:95: DtypeWarning: Columns (8,10,11,45,46,47,48,50,51,52,53,54,55,57,58,59,60,62,63,64,65,66,67,74,75,76,77,78,79,86,87,88,89,90,91,98,99,100,101,102,103,110,111,112,113,114,115,122,123,124,125,126,127,134,135,136,137,138,139,146,147,148,149,150,151,158,159,160,161,162,163,170,171,172,173,174,175,182,183,184,185,186,187,194,196,197,198,199,206,207,208,209,210,211,218,220,221,222,230,231,232,233,234,235,242,243,244,245,246,247,254,255,256,257,258,259) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, sep='\\t', chunksize=CHUNK_SIZE):\n",
      "/tmp/ipykernel_19/2404192709.py:68: RuntimeWarning: divide by zero encountered in log10\n",
      "  return -np.log10(val * 1e-9)\n",
      "/tmp/ipykernel_19/2404192709.py:95: DtypeWarning: Columns (8,9,10,11,20,45,46,47,48,50,51,52,53,54,55,57,58,59,62,63,64,65,66,67,69,70,71,74,75,76,77,78,79,81,82,83,86,87,88,89,90,91,93,94,95,98,99,100,101,102,103,105,106,107,110,111,112,113,114,115,117,118,119,122,123,124,125,126,127,129,130,131,134,135,136,137,138,139,141,142,143,146,147,148,149,150,151,153,154,155,158,159,160,161,162,163,165,166,167,170,171,172,173,174,175,177,178,179,182,183,184,185,186,187,189,190,191,194,195,196,197,198,199,201,202,203,206,207,208,209,210,211,213,214,215,218,219,220,221,222,223,225,226,227,230,231,232,233,234,235,237,238,239,242,243,244,245,246,247,249,250,251,254,255,256,257,258,259,261,262,263,266,267,268,269,270,271,273,274,275,278,279,280,281,282,283,285,286,287,290,291,292,293,294,295,297,298,299,302,303,304,305,306,307,309,310,311,314,315,316,317,318,319,326,327,328,329,330,331,338,339,340,341,342,343,350,351,352,353,354,355,362,363,364,365,366,367,374,375,376,377,378,379,386,387,388,389,390,391,398,399,400,401,402,410,411,412,413,414,415,422,423,424,425,426,427,434,435,436,437,438,439,446,447,448,449,450,451,458,459,460,461,462,463,470,472,473,474,475,482,483,484,485,486,487,494,495,496,497,498,499,506,508,509,510,511,518,520,521,522,530,532,533,534,535,542,543,544,545,546,547,554,556,557,558,559,566,567,568,569,570,571,578,579,580,581,582,583,590,592,593,594,595,602,603,604,605,606,607,614,615,616,617,618,619,626,628,629,630,631) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, sep='\\t', chunksize=CHUNK_SIZE):\n",
      "/tmp/ipykernel_19/2404192709.py:95: DtypeWarning: Columns (8,10,11,45,46,47,48,50,51,52,53,54,55,57,58,59,62,63,64,65,66,67,74,75,76,77,78,79,81,82,83,86,87,88,89,90,91,98,99,100,101,102,103,110,111,112,113,114,115,122,123,124,125,126,127,134,135,136,137,138,139,146,147,148,149,150,151,158,159,160,161,162,163,170,171,172,173,174,175,182,183,184,185,186,187,194,196,197,198,199,206,207,208,209,210,211,218,220,221,222,230,231,232,233,234,235,242,243,244,245,246,247,254,255,256,257,258,259) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, sep='\\t', chunksize=CHUNK_SIZE):\n",
      "/tmp/ipykernel_19/2404192709.py:68: RuntimeWarning: divide by zero encountered in log10\n",
      "  return -np.log10(val * 1e-9)\n",
      "/tmp/ipykernel_19/2404192709.py:95: DtypeWarning: Columns (8,10,11,17,45,46,47,48,50,51,52,53,54,55,57,58,59,62,63,64,65,66,67,74,75,76,77,78,79,86,87,88,89,90,91,98,99,100,101,102,103,110,111,112,113,114,115,122,123,124,125,126,127,134,135,136,137,138,139,146,147,148,149,150,151,158,159,160,161,162,163,170,171,172,173,174,175,182,183,184,185,186,187,194,196,197,198,199,206,207,208,209,210,211,218,220,221,222,230,231,232,233,234,235,242,243,244,245,246,247,254,255,256,257,258,259) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, sep='\\t', chunksize=CHUNK_SIZE):\n",
      "/tmp/ipykernel_19/2404192709.py:68: RuntimeWarning: divide by zero encountered in log10\n",
      "  return -np.log10(val * 1e-9)\n",
      "/tmp/ipykernel_19/2404192709.py:95: DtypeWarning: Columns (8,10,11,17,22,23,33,35,45,46,47,48,50,51,52,53,54,55,57,58,59,62,63,64,65,66,67,74,75,76,77,78,79,86,87,88,89,90,91,98,99,100,101,102,103,110,111,112,113,114,115,122,123,124,125,126,127,134,135,136,137,138,139,146,147,148,149,150,151,158,159,160,161,162,163,170,171,172,173,174,175,182,183,184,185,186,187,194,196,197,198,199,206,207,208,209,210,211,218,220,221,222,230,231,232,233,234,235,242,243,244,245,246,247,254,255,256,257,258,259) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, sep='\\t', chunksize=CHUNK_SIZE):\n",
      "/tmp/ipykernel_19/2404192709.py:95: DtypeWarning: Columns (8,10,35,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, sep='\\t', chunksize=CHUNK_SIZE):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed BindingDB DTI: 32322 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/2404192709.py:124: DtypeWarning: Columns (0,9,10,11,13,14,15,16,17,18,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, chunksize=CHUNK_SIZE):\n",
      "/tmp/ipykernel_19/2404192709.py:124: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, chunksize=CHUNK_SIZE):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AID DTI dataset from /kaggle/input/punched-aid/AID_504832.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/2404192709.py:124: DtypeWarning: Columns (0,9,10,11,13,14,15,16,17,18,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, chunksize=CHUNK_SIZE):\n",
      "/tmp/ipykernel_19/2404192709.py:124: DtypeWarning: Columns (0,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, chunksize=CHUNK_SIZE):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AID DTI dataset from /kaggle/input/punched-aid/AID_504834.csv\n",
      "Processed merged AID DTI: 166636 entries.\n",
      "Processed BindingDB DTI saved to 'processed_bindingdb_dti.csv'\n",
      "Processed AID merged DTI data saved to 'processed_aid_merged_dti.csv'\n",
      "Processed and merged BindingDB and AID DTI data saved to 'processed_merged_all_dti.csv'\n",
      "\n",
      "--- Preprocessing DTI Summary ---\n",
      "BindingDB DTI (log_kd): 32304 entries\n",
      "AID DTI (pAC50): 166636 entries\n",
      "Merged All DTI (log_kd & pAC50): 198512 entries\n",
      "Merged All DTI (only log_kd available): 32304 entries\n",
      "Merged All DTI (only pAC50 available): 166208 entries\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "01_Data_Preprocessing_DTI.py\n",
    "\n",
    "Preprocesses BindingDB and AID datasets for DTI modeling with PfDHODH as the fixed target.\n",
    "- Standardizes SMILES and protein sequence\n",
    "- Handles NaN/inf robustly (removes or fills as appropriate)\n",
    "- Outputs: processed_bindingdb_dti.csv, processed_aid_merged_dti.csv, processed_merged_all_dti.csv\n",
    "\n",
    "Author: [Salau Shina]\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem, RDLogger\n",
    "from rdkit.Chem import SaltRemover\n",
    "\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "# --- Configuration ---\n",
    "BINDINGDB_PATH = '/kaggle/input/bindingdb/BindingDB_All.tsv'\n",
    "AID_PATHS = [\n",
    "    '/kaggle/input/punched-aid/AID_504832.csv',\n",
    "    '/kaggle/input/punched-aid/AID_504834.csv'\n",
    "]\n",
    "TARGET_COLUMN_BINDINGDB = 'Kd (nM)'\n",
    "TARGET_COLUMN_AID = 'Fit_LogAC50'\n",
    "SMILES_COLUMN_BINDINGDB = 'Ligand SMILES'\n",
    "SMILES_COLUMN_AIDs = 'PUBCHEM_EXT_DATASOURCE_SMILES'\n",
    "CHUNK_SIZE = 100_000\n",
    "\n",
    "# Fixed protein target: PfDHODH (Plasmodium falciparum Dihydroorotate Dehydrogenase)\n",
    "PF_DHODH_SEQUENCE = \"MISKLKPQFMFLPKKHILSYCRKDVLNLFEQKFYYTSKRKESNNMKNESLLRLINYNRYYNKIDSNNYYNGGKILSNDRQYIYSPLCEYKKKINDISSYVSVPFKINIRNLGTSNFVNNKKDVLDNDYIYENIKKEKSKHKKIIFLLFVSLFGLYGFFESYNPEFFLYDIFLKFCLKYIDGEICHDLFLLLGKYNILPYDTSNDSIYACTNIKHLDFINPFGVAAGFDKNGVCIDSILKLGFSFIEIGTITPRGQTGNAKPRIFRDVESRSIINSCGFNNMGCDKVTENLILFRKRQEEDKLLSKHIVGVSIGKNKDTVNIVDDLKYCINKIGRYADYIAINVSSPNTPGLRDNQEAGKLKNIILSVKEEIDNLEKNNIMNDESTYNEDNKIVEKKNNFNKNNSHMMKDAKDNFLWFNTTKKKPLVFVKLAPDLNQEQKKEIADVLLETNIDGMIISNTTTQINDIKSFENKKGGVSGAKLKDISTKFICEMYNYTNKQIPIIASGGIFSGLDALEKIEAGASVCQLYSCLVFNGMKSAVQIKRELNHLLYQRGYYNLKEAIGRKHSKS\"\n",
    "\n",
    "# --- Helper Functions ---\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "\n",
    "def standardize_smiles(smiles, remover):\n",
    "    \"\"\"Standardize SMILES by removing salts and canonicalizing.\"\"\"\n",
    "    if pd.isna(smiles):\n",
    "        return None\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol:\n",
    "            mol = remover.StripMol(mol)\n",
    "            return Chem.MolToSmiles(mol, canonical=True)\n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def generate_scaffold(smiles):\n",
    "    \"\"\"Generate Bemis-Murcko scaffold from SMILES.\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    return MurckoScaffold.MurckoSmiles(mol=mol, includeChirality=False)\n",
    "\n",
    "def convert_kd_to_logkd(kd_str):\n",
    "    if pd.isna(kd_str):\n",
    "        return np.nan\n",
    "    s_val = str(kd_str).strip()\n",
    "    try:\n",
    "        if s_val.startswith('<'):\n",
    "            val = float(s_val[1:]) * 0.5\n",
    "        elif s_val.startswith('>'):\n",
    "            val = float(s_val[1:]) * 2.0\n",
    "        else:\n",
    "            val = float(s_val)\n",
    "        return -np.log10(val * 1e-9)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "# --- Scaffold-based K-fold split for DTI (reproducible, correct indices) ---\n",
    "def scaffold_split_kfold(df, n_folds=5, seed=42):\n",
    "    import numpy as np\n",
    "    np.random.seed(seed)\n",
    "    from collections import defaultdict\n",
    "    df_temp = df.dropna(subset=['SMILES']).reset_index()\n",
    "    scaffolds = defaultdict(list)\n",
    "    for idx, smi in enumerate(df_temp['SMILES']):\n",
    "        scaffold = generate_scaffold(smi)\n",
    "        scaffolds[scaffold].append(df_temp.index[idx])\n",
    "    scaffold_sets = sorted(scaffolds.values(), key=lambda x: len(x), reverse=True)\n",
    "    folds = [[] for _ in range(n_folds)]\n",
    "    for i, sset in enumerate(scaffold_sets):\n",
    "        folds[i % n_folds].extend(sset)\n",
    "    all_indices = df.index.tolist()\n",
    "    for k in range(n_folds):\n",
    "        val_idx = folds[k]\n",
    "        train_idx = [i for i in all_indices if i not in val_idx]\n",
    "        yield train_idx, val_idx\n",
    "\n",
    "def process_bindingdb_in_chunks(path, remover):\n",
    "    processed_chunks = []\n",
    "    try:\n",
    "        for chunk in pd.read_csv(path, sep='\\t', chunksize=CHUNK_SIZE):\n",
    "            if SMILES_COLUMN_BINDINGDB not in chunk or TARGET_COLUMN_BINDINGDB not in chunk:\n",
    "                continue\n",
    "            chunk = chunk[[SMILES_COLUMN_BINDINGDB, TARGET_COLUMN_BINDINGDB]].copy()\n",
    "            chunk.rename(columns={SMILES_COLUMN_BINDINGDB: 'SMILES', TARGET_COLUMN_BINDINGDB: 'Kd_nM'}, inplace=True)\n",
    "            chunk['SMILES'] = chunk['SMILES'].apply(lambda x: standardize_smiles(x, remover))\n",
    "            chunk.dropna(subset=['SMILES'], inplace=True)\n",
    "            chunk['log_kd'] = chunk['Kd_nM'].apply(convert_kd_to_logkd)\n",
    "            chunk.dropna(subset=['log_kd'], inplace=True)\n",
    "            chunk.drop(columns=['Kd_nM'], inplace=True)\n",
    "            # Add protein sequence column\n",
    "            chunk['Protein_Sequence'] = PF_DHODH_SEQUENCE\n",
    "            processed_chunks.append(chunk)\n",
    "        if processed_chunks:\n",
    "            df = pd.concat(processed_chunks, ignore_index=True)\n",
    "            df.drop_duplicates(subset=['SMILES', 'Protein_Sequence'], inplace=True)\n",
    "            print(f\"Processed BindingDB DTI: {len(df)} entries.\")\n",
    "            return df\n",
    "        else:\n",
    "            print(\"No valid BindingDB DTI data found.\")\n",
    "            return pd.DataFrame()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: BindingDB file not found at {path}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def process_aid_in_chunks(paths, remover):\n",
    "    processed_chunks = []\n",
    "    for path in paths:\n",
    "        try:\n",
    "            for chunk in pd.read_csv(path, chunksize=CHUNK_SIZE):\n",
    "                if SMILES_COLUMN_AIDs not in chunk or TARGET_COLUMN_AID not in chunk:\n",
    "                    continue\n",
    "                chunk = chunk[[SMILES_COLUMN_AIDs, TARGET_COLUMN_AID]].copy()\n",
    "                chunk.rename(columns={SMILES_COLUMN_AIDs: 'SMILES', TARGET_COLUMN_AID: 'pAC50_raw'}, inplace=True)\n",
    "                chunk['SMILES'] = chunk['SMILES'].apply(lambda x: standardize_smiles(x, remover))\n",
    "                chunk.dropna(subset=['SMILES'], inplace=True)\n",
    "                chunk['pAC50'] = pd.to_numeric(chunk['pAC50_raw'], errors='coerce')\n",
    "                chunk.dropna(subset=['pAC50'], inplace=True)\n",
    "                chunk.drop(columns=['pAC50_raw'], inplace=True)\n",
    "                # Add protein sequence column\n",
    "                chunk['Protein_Sequence'] = PF_DHODH_SEQUENCE\n",
    "                processed_chunks.append(chunk)\n",
    "            print(f\"Processed AID DTI dataset from {path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: AID file not found at {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {path}: {e}\")\n",
    "    if processed_chunks:\n",
    "        df = pd.concat(processed_chunks, ignore_index=True)\n",
    "        df.drop_duplicates(subset=['SMILES', 'Protein_Sequence'], inplace=True)\n",
    "        print(f\"Processed merged AID DTI: {len(df)} entries.\")\n",
    "        return df\n",
    "    else:\n",
    "        print(\"No valid AID DTI data found.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def main():\n",
    "    remover = SaltRemover.SaltRemover()\n",
    "    df_bindingdb = process_bindingdb_in_chunks(BINDINGDB_PATH, remover)\n",
    "    df_aid_merged = process_aid_in_chunks(AID_PATHS, remover)\n",
    "\n",
    "    # Clean processed_bindingdb DTI\n",
    "    if not df_bindingdb.empty:\n",
    "        df_bindingdb = df_bindingdb.replace([np.inf, -np.inf], np.nan)\n",
    "        df_bindingdb = df_bindingdb.dropna(subset=['SMILES', 'log_kd', 'Protein_Sequence'])\n",
    "        df_bindingdb.to_csv('processed_bindingdb_dti.csv', index=False)\n",
    "        print(\"Processed BindingDB DTI saved to 'processed_bindingdb_dti.csv'\")\n",
    "    # Clean processed_aid_merged DTI\n",
    "    if not df_aid_merged.empty:\n",
    "        df_aid_merged = df_aid_merged.replace([np.inf, -np.inf], np.nan)\n",
    "        df_aid_merged = df_aid_merged.dropna(subset=['SMILES', 'pAC50', 'Protein_Sequence'])\n",
    "        df_aid_merged.to_csv('processed_aid_merged_dti.csv', index=False)\n",
    "        print(\"Processed AID merged DTI data saved to 'processed_aid_merged_dti.csv'\")\n",
    "\n",
    "    # Multi-task learning: Merge for cross-validation and fine-tuning\n",
    "    if not df_bindingdb.empty or not df_aid_merged.empty:\n",
    "        bindingdb_for_merge = df_bindingdb[['SMILES', 'Protein_Sequence', 'log_kd']].copy()\n",
    "        bindingdb_for_merge['pAC50'] = np.nan\n",
    "        aid_for_merge = df_aid_merged[['SMILES', 'Protein_Sequence', 'pAC50']].copy()\n",
    "        aid_for_merge['log_kd'] = np.nan\n",
    "        bindingdb_for_merge = bindingdb_for_merge[['SMILES', 'Protein_Sequence', 'log_kd', 'pAC50']]\n",
    "        aid_for_merge = aid_for_merge[['SMILES', 'Protein_Sequence', 'log_kd', 'pAC50']]\n",
    "        df_merged_all = pd.concat([bindingdb_for_merge, aid_for_merge], ignore_index=True)\n",
    "        # Remove rows where both log_kd and pAC50 are missing or inf\n",
    "        df_merged_all = df_merged_all.replace([np.inf, -np.inf], np.nan)\n",
    "        df_merged_all = df_merged_all[(df_merged_all['log_kd'].notna()) | (df_merged_all['pAC50'].notna())]\n",
    "        # Do NOT fill NaN targets with 0.0; keep NaN for masking in multitask learning\n",
    "        df_merged_all.drop_duplicates(subset=['SMILES', 'Protein_Sequence'], inplace=True)\n",
    "        df_merged_all.to_csv('processed_merged_all_dti.csv', index=False)\n",
    "        print(\"Processed and merged BindingDB and AID DTI data saved to 'processed_merged_all_dti.csv'\")\n",
    "\n",
    "    print(\"\\n--- Preprocessing DTI Summary ---\")\n",
    "    if not df_bindingdb.empty:\n",
    "        print(f\"BindingDB DTI (log_kd): {len(df_bindingdb)} entries\")\n",
    "    if not df_aid_merged.empty:\n",
    "        print(f\"AID DTI (pAC50): {len(df_aid_merged)} entries\")\n",
    "    if 'df_merged_all' in locals():\n",
    "        print(f\"Merged All DTI (log_kd & pAC50): {len(df_merged_all)} entries\")\n",
    "        print(f\"Merged All DTI (only log_kd available): {df_merged_all['log_kd'].notna().sum()} entries\")\n",
    "        print(f\"Merged All DTI (only pAC50 available): {df_merged_all['pAC50'].notna().sum()} entries\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7293458,
     "sourceId": 11625475,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7576939,
     "sourceId": 12041030,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3323.12941,
   "end_time": "2025-07-05T13:06:37.607754",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-05T12:11:14.478344",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

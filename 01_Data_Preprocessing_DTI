{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb246434",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-16T21:39:13.462328Z",
     "iopub.status.busy": "2025-07-16T21:39:13.462050Z",
     "iopub.status.idle": "2025-07-16T21:41:04.555336Z",
     "shell.execute_reply": "2025-07-16T21:41:04.554403Z"
    },
    "papermill": {
     "duration": 111.097695,
     "end_time": "2025-07-16T21:41:04.557111",
     "exception": false,
     "start_time": "2025-07-16T21:39:13.459416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install --quiet pandas numpy scikit-learn torch transformers rdkit-pypi fair-esm optuna matplotlib seaborn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0abc8549",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T21:41:04.644362Z",
     "iopub.status.busy": "2025-07-16T21:41:04.644075Z",
     "iopub.status.idle": "2025-07-16T21:47:54.489980Z",
     "shell.execute_reply": "2025-07-16T21:47:54.489182Z"
    },
    "papermill": {
     "duration": 409.890801,
     "end_time": "2025-07-16T21:47:54.491225",
     "exception": false,
     "start_time": "2025-07-16T21:41:04.600424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed BindingDB DTI: 0 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/1824540485.py:92: DtypeWarning: Columns (0,9,10,11,13,14,15,16,17,18,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  pd.read_csv(path)\n",
      "/tmp/ipykernel_19/1824540485.py:92: DtypeWarning: Columns (0,9,10,11,13,14,15,16,17,18,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  pd.read_csv(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed merged AID DTI: 166636 entries.\n",
      "No BindingDB DTI data to save.\n",
      "Processed AID merged DTI data saved to 'processed_aid_merged_dti.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/1824540485.py:139: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_merged_all = pd.concat([bindingdb_for_merge, aid_for_merge], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and merged BindingDB and AID DTI data saved to 'processed_merged_all_dti.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem, RDLogger\n",
    "from rdkit.Chem import SaltRemover\n",
    "\n",
    "# Disable RDKit logging\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "# Configuration\n",
    "BINDINGDB_PATH = '/kaggle/input/bindingdb/BindingDB_All.tsv'\n",
    "AID_PATHS = [\n",
    "    '/kaggle/input/punched-aid/AID_504832.csv',\n",
    "    '/kaggle/input/punched-aid/AID_504834.csv'\n",
    "]\n",
    "TARGET_COLUMN_BINDINGDB = 'Kd (nM)'\n",
    "SMILES_COLUMN_BINDINGDB = 'Ligand SMILES'\n",
    "TARGET_SEQUENCE_COLUMN = 'BindingDB Target Chain Sequence'  # Updated column name\n",
    "SMILES_COLUMN_AIDs = 'PUBCHEM_EXT_DATASOURCE_SMILES'\n",
    "TARGET_COLUMN_AID = 'Fit_LogAC50'\n",
    "CHUNK_SIZE = 100_000\n",
    "\n",
    "# Fixed PfDHODH protein sequence\n",
    "PF_DHODH_SEQUENCE = \"MISKLKPQFMFLPKKHILSYCRKDVLNLFEQKFYYTSKRKESNNMKNESLLRLINYNRYYNKIDSNNYYNGGKILSNDRQYIYSPLCEYKKKINDISSYVSVPFKINIRNLGTSNFVNNKKDVLDNDYIYENIKKEKSKHKKIIFLLFVSLFGLYGFFESYNPEFFLYDIFLKFCLKYIDGEICHDLFLLLGKYNILPYDTSNDSIYACTNIKHLDFINPFGVAAGFDKNGVCIDSILKLGFSFIEIGTITPRGQTGNAKPRIFRDVESRSIINSCGFNNMGCDKVTENLILFRKRQEEDKLLSKHIVGVSIGKNKDTVNIVDDLKYCINKIGRYADYIAINVSSPNTPGLRDNQEAGKLKNIILSVKEEIDNLEKNNIMNDESTYNEDNKIVEKKNNFNKNNSHMMKDAKDNFLWFNTTKKKPLVFVKLAPDLNQEQKKEIADVLLETNIDGMIISNTTTQINDIKSFENKKGGVSGAKLKDISTKFICEMYNYTNKQIPIIASGGIFSGLDALEKIEAGASVCQLYSCLVFNGMKSAVQIKRELNHLLYQRGYYNLKEAIGRKHSKS\"\n",
    "\n",
    "# Helper Functions\n",
    "def standardize_smiles(smiles, remover):\n",
    "    \"\"\"Standardize SMILES strings by canonicalizing and removing salts.\"\"\"\n",
    "    if pd.isna(smiles):\n",
    "        return None\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol:\n",
    "            mol = remover.StripMol(mol)\n",
    "            return Chem.MolToSmiles(mol, canonical=True)\n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def convert_kd_to_logkd(kd_str):\n",
    "    \"\"\"Convert Kd (nM) to log scale, handling qualifiers.\"\"\"\n",
    "    if pd.isna(kd_str):\n",
    "        return np.nan\n",
    "    s_val = str(kd_str).strip()\n",
    "    try:\n",
    "        if s_val.startswith('<'):\n",
    "            val = float(s_val[1:]) * 0.5\n",
    "        elif s_val.startswith('>'):\n",
    "            val = float(s_val[1:]) * 2.0\n",
    "        else:\n",
    "            val = float(s_val)\n",
    "        return -np.log10(val * 1e-9)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "# Process BindingDB in Chunks\n",
    "def process_bindingdb_in_chunks(path, remover, chunk_size=CHUNK_SIZE):\n",
    "    \"\"\"Process BindingDB dataset, filtering for PfDHODH.\"\"\"\n",
    "    processed_chunks = []\n",
    "    columns_to_load = [SMILES_COLUMN_BINDINGDB, TARGET_COLUMN_BINDINGDB, TARGET_SEQUENCE_COLUMN]\n",
    "    for chunk in pd.read_csv(path, sep='\\t', chunksize=chunk_size, usecols=columns_to_load, low_memory=False):\n",
    "        # Filter for PfDHODH target\n",
    "        chunk = chunk[chunk[TARGET_SEQUENCE_COLUMN] == PF_DHODH_SEQUENCE]\n",
    "        if chunk.empty:\n",
    "            continue\n",
    "        # Select and rename relevant columns\n",
    "        chunk = chunk[[SMILES_COLUMN_BINDINGDB, TARGET_COLUMN_BINDINGDB]].copy()\n",
    "        chunk.rename(columns={SMILES_COLUMN_BINDINGDB: 'SMILES', TARGET_COLUMN_BINDINGDB: 'Kd_nM'}, inplace=True)\n",
    "        # Standardize SMILES\n",
    "        chunk['SMILES'] = chunk['SMILES'].apply(lambda x: standardize_smiles(x, remover))\n",
    "        chunk.dropna(subset=['SMILES'], inplace=True)\n",
    "        # Convert Kd to log_kd\n",
    "        chunk['log_kd'] = chunk['Kd_nM'].apply(convert_kd_to_logkd)\n",
    "        chunk.dropna(subset=['log_kd'], inplace=True)\n",
    "        chunk.drop(columns=['Kd_nM'], inplace=True)\n",
    "        # Add protein sequence\n",
    "        chunk['Protein_Sequence'] = PF_DHODH_SEQUENCE\n",
    "        processed_chunks.append(chunk)\n",
    "    if processed_chunks:\n",
    "        df = pd.concat(processed_chunks, ignore_index=True)\n",
    "        df.drop_duplicates(subset=['SMILES', 'Protein_Sequence'], inplace=True)\n",
    "        print(f\"Processed BindingDB DTI: {len(df)} entries\")\n",
    "        return df\n",
    "    else:\n",
    "        print(\"No valid BindingDB DTI data found for PfDHODH.\")\n",
    "        return pd.DataFrame(columns=['SMILES', 'Protein_Sequence', 'log_kd'])\n",
    "\n",
    "# Process AID Datasets\n",
    "def process_aid_in_chunks(paths, remover):\n",
    "    \"\"\"Process AID datasets and merge them.\"\"\"\n",
    "    processed_chunks = pd.concat(\n",
    "        [\n",
    "            pd.read_csv(path)\n",
    "            .rename(columns={SMILES_COLUMN_AIDs: 'SMILES', TARGET_COLUMN_AID: 'pAC50'})\n",
    "            .assign(SMILES=lambda x: x['SMILES'].apply(lambda s: standardize_smiles(s, remover)))\n",
    "            .dropna(subset=['SMILES'])\n",
    "            .assign(pAC50=lambda x: pd.to_numeric(x['pAC50'], errors='coerce'))\n",
    "            .dropna(subset=['pAC50'])\n",
    "            .assign(Protein_Sequence=PF_DHODH_SEQUENCE)[['SMILES', 'Protein_Sequence', 'pAC50']]\n",
    "            for path in paths\n",
    "        ],\n",
    "        ignore_index=True\n",
    "    ).drop_duplicates(subset=['SMILES', 'Protein_Sequence'])\n",
    "    print(f\"Processed merged AID DTI: {len(processed_chunks)} entries.\")\n",
    "    return processed_chunks if not processed_chunks.empty else pd.DataFrame(columns=['SMILES', 'Protein_Sequence', 'pAC50'])\n",
    "\n",
    "# Main Execution\n",
    "def main():\n",
    "    \"\"\"Preprocess datasets and save output files.\"\"\"\n",
    "    remover = SaltRemover.SaltRemover()\n",
    "\n",
    "    # Process datasets\n",
    "    df_bindingdb = process_bindingdb_in_chunks(BINDINGDB_PATH, remover)\n",
    "    df_aid_merged = process_aid_in_chunks(AID_PATHS, remover)\n",
    "\n",
    "    # Save BindingDB data\n",
    "    if not df_bindingdb.empty:\n",
    "        df_bindingdb.to_csv('processed_bindingdb_dti.csv', index=False)\n",
    "        print(\"Processed BindingDB DTI saved to 'processed_bindingdb_dti.csv'\")\n",
    "    else:\n",
    "        print(\"No BindingDB DTI data to save.\")\n",
    "\n",
    "    # Save AID merged data\n",
    "    if not df_aid_merged.empty:\n",
    "        df_aid_merged.to_csv('processed_aid_merged_dti.csv', index=False)\n",
    "        print(\"Processed AID merged DTI data saved to 'processed_aid_merged_dti.csv'\")\n",
    "    else:\n",
    "        print(\"No AID DTI data to save.\")\n",
    "\n",
    "    # Merge datasets\n",
    "    columns = ['SMILES', 'Protein_Sequence', 'log_kd', 'pAC50']\n",
    "    bindingdb_for_merge = df_bindingdb[['SMILES', 'Protein_Sequence', 'log_kd']].copy() if not df_bindingdb.empty else pd.DataFrame(columns=columns)\n",
    "    if not bindingdb_for_merge.empty:\n",
    "        bindingdb_for_merge['pAC50'] = np.nan\n",
    "\n",
    "    aid_for_merge = df_aid_merged[['SMILES', 'Protein_Sequence', 'pAC50']].copy() if not df_aid_merged.empty else pd.DataFrame(columns=columns)\n",
    "    if not aid_for_merge.empty:\n",
    "        aid_for_merge['log_kd'] = np.nan\n",
    "\n",
    "    df_merged_all = pd.concat([bindingdb_for_merge, aid_for_merge], ignore_index=True)\n",
    "    if not df_merged_all.empty:\n",
    "        df_merged_all.to_csv('processed_merged_all_dti.csv', index=False)\n",
    "        print(\"Processed and merged BindingDB and AID DTI data saved to 'processed_merged_all_dti.csv'\")\n",
    "    else:\n",
    "        print(\"No data to merge.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7293458,
     "sourceId": 11625475,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7576939,
     "sourceId": 12041030,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 526.516486,
   "end_time": "2025-07-16T21:47:55.052272",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-16T21:39:08.535786",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

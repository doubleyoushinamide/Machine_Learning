{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb5f497a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-28T13:55:30.760344Z",
     "iopub.status.busy": "2025-07-28T13:55:30.759785Z",
     "iopub.status.idle": "2025-07-28T13:56:44.905486Z",
     "shell.execute_reply": "2025-07-28T13:56:44.904742Z"
    },
    "papermill": {
     "duration": 74.149929,
     "end_time": "2025-07-28T13:56:44.907017",
     "exception": false,
     "start_time": "2025-07-28T13:55:30.757088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q torch torchvision opencv-python numpy Pillow matplotlib albumentations tqdm  scikit-learn tensorboard optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f2d9cd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T13:56:44.949440Z",
     "iopub.status.busy": "2025-07-28T13:56:44.949171Z",
     "iopub.status.idle": "2025-07-28T13:56:44.956105Z",
     "shell.execute_reply": "2025-07-28T13:56:44.955531Z"
    },
    "papermill": {
     "duration": 0.028567,
     "end_time": "2025-07-28T13:56:44.957172",
     "exception": false,
     "start_time": "2025-07-28T13:56:44.928605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Configuration file for Facial Detection System\n",
    "Optimized for Kaggle hardware constraints\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "class Config:\n",
    "    # Hardware constraints for Kaggle\n",
    "    GPU_MEMORY_LIMIT = 14  # GB (leaving 2GB buffer)\n",
    "    RAM_LIMIT = 28  # GB (leaving 4GB buffer)\n",
    "    CPU_CORES = 4\n",
    "    \n",
    "    # Dataset settings\n",
    "    DATASET_NAME = \"wider-face\"  # WIDER FACE dataset\n",
    "    TRAIN_SPLIT = 0.8\n",
    "    VAL_SPLIT = 0.2\n",
    "    IMAGE_SIZE = (416, 416)  # Optimized for real-time processing\n",
    "    BATCH_SIZE = 16  # Optimized for P100 GPU memory\n",
    "    \n",
    "    # Data augmentation settings\n",
    "    AUGMENTATION_PROBABILITY = 0.8\n",
    "    ROTATION_RANGE = 15\n",
    "    BRIGHTNESS_RANGE = 0.2\n",
    "    CONTRAST_RANGE = 0.2\n",
    "    HORIZONTAL_FLIP_PROB = 0.5\n",
    "    VERTICAL_FLIP_PROB = 0.0  # Keep faces upright\n",
    "    \n",
    "    # Model settings\n",
    "    MODEL_TYPE = \"mobilenet_v3\"  # Lightweight and efficient\n",
    "    PRETRAINED = True\n",
    "    NUM_CLASSES = 1  # Face detection only\n",
    "    CONFIDENCE_THRESHOLD = 0.5\n",
    "    NMS_THRESHOLD = 0.4\n",
    "    \n",
    "    # Training settings\n",
    "    EPOCHS = 50\n",
    "    LEARNING_RATE = 0.001\n",
    "    WEIGHT_DECAY = 1e-4\n",
    "    SCHEDULER_STEP_SIZE = 10\n",
    "    SCHEDULER_GAMMA = 0.5\n",
    "    \n",
    "    # Real-time detection settings\n",
    "    FPS_TARGET = 15  # Process every 4th frame at 60fps\n",
    "    FRAME_SKIP = 4\n",
    "    DETECTION_INTERVAL = 3  # frames between detections\n",
    "    \n",
    "    # Paths\n",
    "    DATA_DIR = \"/kaggle/input/input-images-davido\"\n",
    "    MODELS_DIR = \"./models\"\n",
    "    LOGS_DIR = \"./logs\"\n",
    "    OUTPUT_DIR = \"./output\"\n",
    "    \n",
    "    # Create directories\n",
    "    @staticmethod\n",
    "    def create_directories():\n",
    "        \"\"\"Create necessary directories\"\"\"\n",
    "        dirs = [Config.DATA_DIR, Config.MODELS_DIR, Config.LOGS_DIR, Config.OUTPUT_DIR]\n",
    "        for dir_path in dirs:\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "    \n",
    "    # Model export settings\n",
    "    EXPORT_FORMAT = \"pt\"  # PyTorch format for local use\n",
    "    MODEL_FILENAME = \"face_detection_model.pt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "909f43ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T13:56:44.994779Z",
     "iopub.status.busy": "2025-07-28T13:56:44.994582Z",
     "iopub.status.idle": "2025-07-28T13:56:55.167678Z",
     "shell.execute_reply": "2025-07-28T13:56:55.166810Z"
    },
    "papermill": {
     "duration": 10.193519,
     "end_time": "2025-07-28T13:56:55.168919",
     "exception": false,
     "start_time": "2025-07-28T13:56:44.975400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Section 1: Data Preparation ===\n",
      "Dataset not found. Please upload the WIDER FACE dataset files to the data directory using the Kaggle notebook interface.\n",
      "Data preparation failed. Please check dataset availability.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Section 1: Data Preparation\n",
    "Loads Kaggle dataset and implements data augmentation for facial detection\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from config import Config\n",
    "\n",
    "class FaceDetectionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset class for face detection\n",
    "    Handles WIDER FACE dataset format with comprehensive data augmentation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, annotation_file, transform=None, is_training=True):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.is_training = is_training\n",
    "        \n",
    "        # Load annotations\n",
    "        self.annotations = self._load_annotations(annotation_file)\n",
    "        self.image_paths = list(self.annotations.keys())\n",
    "        \n",
    "        print(f\"Loaded {len(self.image_paths)} images for {'training' if is_training else 'validation'}\")\n",
    "    \n",
    "    def _load_annotations(self, annotation_file):\n",
    "        \"\"\"Load WIDER FACE annotation format\"\"\"\n",
    "        annotations = {}\n",
    "        \n",
    "        with open(annotation_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            # Image path\n",
    "            image_path = lines[i].strip()\n",
    "            i += 1\n",
    "            \n",
    "            # Number of faces\n",
    "            num_faces = int(lines[i].strip())\n",
    "            i += 1\n",
    "            \n",
    "            faces = []\n",
    "            for _ in range(num_faces):\n",
    "                if i >= len(lines):\n",
    "                    break\n",
    "                    \n",
    "                # Face bounding box: x, y, w, h, blur, expression, illumination, invalid, occlusion, pose\n",
    "                face_data = lines[i].strip().split()\n",
    "                i += 1\n",
    "                \n",
    "                if len(face_data) >= 4:\n",
    "                    x, y, w, h = map(float, face_data[:4])\n",
    "                    # Convert to normalized coordinates\n",
    "                    faces.append([x, y, w, h])\n",
    "            \n",
    "            if faces:  # Only include images with faces\n",
    "                annotations[image_path] = faces\n",
    "        \n",
    "        return annotations\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        full_path = os.path.join(self.data_dir, image_path)\n",
    "        \n",
    "        # Load image\n",
    "        image = cv2.imread(full_path)\n",
    "        if image is None:\n",
    "            # Handle missing images\n",
    "            image = np.zeros((Config.IMAGE_SIZE[1], Config.IMAGE_SIZE[0], 3), dtype=np.uint8)\n",
    "        \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Get face annotations\n",
    "        faces = self.annotations[image_path]\n",
    "        \n",
    "        # Convert to albumentations format\n",
    "        bboxes = []\n",
    "        for face in faces:\n",
    "            x, y, w, h = face\n",
    "            # Convert to [x_min, y_min, x_max, y_max] format\n",
    "            bboxes.append([x, y, x + w, y + h])\n",
    "        \n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, bboxes=bboxes, class_labels=[1] * len(bboxes))\n",
    "            image = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "            labels = transformed['class_labels']\n",
    "        else:\n",
    "            labels = [1] * len(bboxes)\n",
    "        \n",
    "        # Convert to tensor format\n",
    "        if isinstance(image, np.ndarray):\n",
    "            image = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255.0\n",
    "        \n",
    "        # Convert bboxes to tensor\n",
    "        if bboxes:\n",
    "            bboxes = torch.tensor(bboxes, dtype=torch.float32)\n",
    "            labels = torch.tensor(labels, dtype=torch.long)\n",
    "        else:\n",
    "            bboxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros(0, dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'bboxes': bboxes,\n",
    "            'labels': labels,\n",
    "            'image_path': image_path\n",
    "        }\n",
    "\n",
    "class DataAugmentation:\n",
    "    \"\"\"\n",
    "    Comprehensive data augmentation pipeline for face detection\n",
    "    Optimized for limited training data scenarios\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_training_transforms():\n",
    "        \"\"\"Get training transforms with heavy augmentation\"\"\"\n",
    "        return A.Compose([\n",
    "            # Geometric transformations\n",
    "            A.RandomRotate90(p=0.3),\n",
    "            A.Flip(p=Config.HORIZONTAL_FLIP_PROB),\n",
    "            A.Rotate(limit=Config.ROTATION_RANGE, p=0.5),\n",
    "            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=10, p=0.5),\n",
    "            \n",
    "            # Color and brightness augmentations\n",
    "            A.RandomBrightnessContrast(\n",
    "                brightness_limit=Config.BRIGHTNESS_RANGE,\n",
    "                contrast_limit=Config.CONTRAST_RANGE,\n",
    "                p=0.5\n",
    "            ),\n",
    "            A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "            A.RGBShift(r_shift_limit=20, g_shift_limit=20, b_shift_limit=20, p=0.3),\n",
    "            \n",
    "            # Noise and blur\n",
    "            A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
    "            A.GaussianBlur(blur_limit=(3, 7), p=0.3),\n",
    "            A.MotionBlur(blur_limit=7, p=0.2),\n",
    "            \n",
    "            # Perspective and distortion\n",
    "            A.Perspective(scale=(0.05, 0.1), p=0.3),\n",
    "            A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.2),\n",
    "            \n",
    "            # Weather effects (simulate different lighting conditions)\n",
    "            A.RandomShadow(p=0.2),\n",
    "            A.RandomSunFlare(flare_roi=(0, 0, 1, 0.5), angle_lower=0, angle_upper=1, p=0.1),\n",
    "            \n",
    "            # Resize and normalize\n",
    "            A.Resize(height=Config.IMAGE_SIZE[1], width=Config.IMAGE_SIZE[0]),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_validation_transforms():\n",
    "        \"\"\"Get validation transforms (minimal augmentation)\"\"\"\n",
    "        return A.Compose([\n",
    "            A.Resize(height=Config.IMAGE_SIZE[1], width=Config.IMAGE_SIZE[0]),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n",
    "\n",
    "class DatasetManager:\n",
    "    \"\"\"\n",
    "    Manages dataset preparation and loading\n",
    "    Assumes dataset is already uploaded to the data directory in Kaggle\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        Config.create_directories()\n",
    "    \n",
    "    def prepare_data_loaders(self):\n",
    "        \"\"\"Prepare training and validation data loaders\"\"\"\n",
    "        \n",
    "        # Check if dataset exists\n",
    "        train_annotations = os.path.join(Config.DATA_DIR, \"wider_face_train_bbx_gt.txt\")\n",
    "        val_annotations = os.path.join(Config.DATA_DIR, \"wider_face_val_bbx_gt.txt\")\n",
    "        \n",
    "        if not os.path.exists(train_annotations):\n",
    "            print(\"Dataset not found. Please upload the WIDER FACE dataset files to the data directory using the Kaggle notebook interface.\")\n",
    "            return None, None\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = FaceDetectionDataset(\n",
    "            data_dir=os.path.join(Config.DATA_DIR, \"WIDER_train\"),\n",
    "            annotation_file=train_annotations,\n",
    "            transform=DataAugmentation.get_training_transforms(),\n",
    "            is_training=True\n",
    "        )\n",
    "        \n",
    "        val_dataset = FaceDetectionDataset(\n",
    "            data_dir=os.path.join(Config.DATA_DIR, \"WIDER_val\"),\n",
    "            annotation_file=val_annotations,\n",
    "            transform=DataAugmentation.get_validation_transforms(),\n",
    "            is_training=False\n",
    "        )\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=Config.BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=min(Config.CPU_CORES, 4),\n",
    "            pin_memory=True,\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=Config.BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=min(Config.CPU_CORES, 4),\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Created data loaders:\")\n",
    "        print(f\"  Training: {len(train_loader)} batches\")\n",
    "        print(f\"  Validation: {len(val_loader)} batches\")\n",
    "        \n",
    "        return train_loader, val_loader\n",
    "    \n",
    "    def visualize_augmentation(self, num_samples=5):\n",
    "        \"\"\"Visualize data augmentation effects\"\"\"\n",
    "        train_loader, _ = self.prepare_data_loaders()\n",
    "        \n",
    "        if train_loader is None:\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(num_samples, 3, figsize=(15, 5*num_samples))\n",
    "        \n",
    "        for i, batch in enumerate(train_loader):\n",
    "            if i >= num_samples:\n",
    "                break\n",
    "            \n",
    "            image = batch['image'][0]\n",
    "            bboxes = batch['bboxes'][0]\n",
    "            \n",
    "            # Convert tensor to numpy for visualization\n",
    "            image_np = image.permute(1, 2, 0).numpy()\n",
    "            image_np = (image_np * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]) * 255\n",
    "            image_np = np.clip(image_np, 0, 255).astype(np.uint8)\n",
    "            \n",
    "            # Draw bounding boxes\n",
    "            for bbox in bboxes:\n",
    "                x1, y1, x2, y2 = bbox.numpy()\n",
    "                cv2.rectangle(image_np, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "            \n",
    "            axes[i, 0].imshow(image_np)\n",
    "            axes[i, 0].set_title(f'Augmented Sample {i+1}')\n",
    "            axes[i, 0].axis('off')\n",
    "            \n",
    "            # Show original image for comparison\n",
    "            if i == 0:\n",
    "                axes[i, 1].text(0.5, 0.5, 'Original Image\\n(if available)', \n",
    "                               ha='center', va='center', transform=axes[i, 1].transAxes)\n",
    "                axes[i, 1].set_title('Original')\n",
    "                axes[i, 1].axis('off')\n",
    "            \n",
    "            # Show augmentation info\n",
    "            axes[i, 2].text(0.1, 0.5, f'Faces detected: {len(bboxes)}\\nImage size: {image.shape}', \n",
    "                           transform=axes[i, 2].transAxes, fontsize=10)\n",
    "            axes[i, 2].set_title('Info')\n",
    "            axes[i, 2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(Config.OUTPUT_DIR, 'data_augmentation_samples.png'))\n",
    "        plt.show()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function for data preparation testing\"\"\"\n",
    "    print(\"=== Section 1: Data Preparation ===\")\n",
    "    \n",
    "    # Initialize dataset manager\n",
    "    dataset_manager = DatasetManager()\n",
    "    \n",
    "    # Prepare data loaders\n",
    "    train_loader, val_loader = dataset_manager.prepare_data_loaders()\n",
    "    \n",
    "    if train_loader is not None:\n",
    "        print(\"Data preparation completed successfully!\")\n",
    "        \n",
    "        # Visualize augmentation effects\n",
    "        print(\"Generating augmentation visualization...\")\n",
    "        dataset_manager.visualize_augmentation()\n",
    "        \n",
    "        # Test data loading\n",
    "        print(\"Testing data loading...\")\n",
    "        for batch in train_loader:\n",
    "            print(f\"Batch shape: {batch['image'].shape}\")\n",
    "            print(f\"Number of faces in batch: {sum(len(bboxes) for bboxes in batch['bboxes'])}\")\n",
    "            break\n",
    "    else:\n",
    "        print(\"Data preparation failed. Please check dataset availability.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7959389,
     "sourceId": 12601328,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 89.739026,
   "end_time": "2025-07-28T13:56:56.408195",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-28T13:55:26.669169",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

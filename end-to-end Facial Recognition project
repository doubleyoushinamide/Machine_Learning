{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41a82b1c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-30T13:37:30.361413Z",
     "iopub.status.busy": "2025-07-30T13:37:30.361150Z",
     "iopub.status.idle": "2025-07-30T13:38:46.523218Z",
     "shell.execute_reply": "2025-07-30T13:38:46.522461Z"
    },
    "papermill": {
     "duration": 76.166624,
     "end_time": "2025-07-30T13:38:46.524814",
     "exception": false,
     "start_time": "2025-07-30T13:37:30.358190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q torch torchvision opencv-python numpy Pillow matplotlib albumentations tqdm  scikit-learn tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9788485",
   "metadata": {
    "papermill": {
     "duration": 0.019463,
     "end_time": "2025-07-30T13:38:46.565589",
     "exception": false,
     "start_time": "2025-07-30T13:38:46.546126",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e08e35c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T13:38:46.604530Z",
     "iopub.status.busy": "2025-07-30T13:38:46.604230Z",
     "iopub.status.idle": "2025-07-30T13:38:46.617175Z",
     "shell.execute_reply": "2025-07-30T13:38:46.616474Z"
    },
    "papermill": {
     "duration": 0.033834,
     "end_time": "2025-07-30T13:38:46.618379",
     "exception": false,
     "start_time": "2025-07-30T13:38:46.584545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Configuration file for Face Recognition System\n",
    "Contains all the key parameters for easy tuning and customization.\n",
    "\"\"\"\n",
    "\n",
    "# Model Configuration\n",
    "MODEL_CONFIG = {\n",
    "    'model_name': 'mobilenet_v2',\n",
    "    'num_classes': 2,\n",
    "    'input_size': 224,\n",
    "    'dropout_rate': 0.5,\n",
    "    'pretrained': True\n",
    "}\n",
    "\n",
    "# Training Configuration\n",
    "TRAINING_CONFIG = {\n",
    "    'batch_size': 16,  # Optimized for 16GB GPU\n",
    "    'num_epochs': 20,\n",
    "    'early_stopping_patience': 7,\n",
    "    'learning_rate_range': (1e-5, 1e-2),  # For Optuna optimization\n",
    "    'weight_decay_range': (1e-5, 1e-2),   # For Optuna optimization\n",
    "    'optuna_trials': 10,\n",
    "    'validation_split': 0.2,\n",
    "    'random_seed': 42\n",
    "}\n",
    "\n",
    "# Data Augmentation Configuration\n",
    "AUGMENTATION_CONFIG = {\n",
    "    'train_transforms': [\n",
    "        ('Resize', {'height': 224, 'width': 224}),\n",
    "        ('HorizontalFlip', {'p': 0.5}),\n",
    "        ('VerticalFlip', {'p': 0.1}),\n",
    "        ('RandomRotate90', {'p': 0.3}),\n",
    "        ('Rotate', {'limit': 15, 'p': 0.5}),\n",
    "        ('RandomBrightnessContrast', {'brightness_limit': 0.2, 'contrast_limit': 0.2, 'p': 0.5}),\n",
    "        ('HueSaturationValue', {'hue_shift_limit': 20, 'sat_shift_limit': 30, 'val_shift_limit': 20, 'p': 0.5}),\n",
    "        ('GaussNoise', {'var_limit': (10.0, 50.0), 'p': 0.3}),\n",
    "        ('GaussianBlur', {'blur_limit': (3, 7), 'p': 0.2}),\n",
    "        ('ElasticTransform', {'alpha': 1, 'sigma': 50, 'alpha_affine': 50, 'p': 0.2}),\n",
    "        ('GridDistortion', {'num_steps': 5, 'distort_limit': 0.3, 'p': 0.2}),\n",
    "        ('OpticalDistortion', {'distort_limit': 0.2, 'shift_limit': 0.15, 'p': 0.2}),\n",
    "        ('CoarseDropout', {'max_holes': 8, 'max_height': 32, 'max_width': 32, 'p': 0.3}),\n",
    "        ('Normalize', {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}),\n",
    "    ],\n",
    "    'val_transforms': [\n",
    "        ('Resize', {'height': 224, 'width': 224}),\n",
    "        ('Normalize', {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}),\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Inference Configuration\n",
    "INFERENCE_CONFIG = {\n",
    "    'confidence_threshold': 0.9,  # Threshold for artist detection\n",
    "    'face_detection_confidence': 0.9,  # MTCNN confidence threshold\n",
    "    'min_face_size': 20,  # Minimum face size for detection\n",
    "    'scale_factor': 0.709,  # MTCNN scale factor\n",
    "    'video_frame_skip': 3,  # Process every Nth frame for efficiency\n",
    "}\n",
    "\n",
    "# Data Paths\n",
    "DATA_PATHS = {\n",
    "    'kaggle_artist_path': '/kaggle/input/input-data/Davido',\n",
    "    'kaggle_others_path': '/kaggle/input/input-data/Unknown',\n",
    "    'model_save_path': 'artist_model_best.pth',\n",
    "    'final_model_path': 'artist_model_final.pth',\n",
    "    'training_curves_path': 'training_curves.png'\n",
    "}\n",
    "\n",
    "# Device Configuration\n",
    "DEVICE_CONFIG = {\n",
    "    'use_gpu': True,\n",
    "    'num_workers': 2,  # For data loading\n",
    "    'pin_memory': True,\n",
    "}\n",
    "\n",
    "# Video Processing Configuration\n",
    "VIDEO_CONFIG = {\n",
    "    'output_fps': 30,\n",
    "    'output_codec': 'mp4v',\n",
    "    'draw_boxes': True,\n",
    "    'show_probabilities': True,\n",
    "    'box_thickness': 2,\n",
    "    'text_scale': 0.5,\n",
    "    'text_thickness': 2,\n",
    "}\n",
    "\n",
    "# Colors for visualization\n",
    "COLORS = {\n",
    "    'artist_face': (0, 255, 0),    # Green for artist\n",
    "    'other_face': (0, 0, 255),     # Red for others\n",
    "    'text_color': (255, 255, 255), # White text\n",
    "}\n",
    "\n",
    "# Logging Configuration\n",
    "LOGGING_CONFIG = {\n",
    "    'log_level': 'INFO',\n",
    "    'log_format': '%(asctime)s - %(levelname)s - %(message)s',\n",
    "    'save_logs': True,\n",
    "    'log_file': 'face_recognition.log'\n",
    "}\n",
    "\n",
    "# Performance Configuration\n",
    "PERFORMANCE_CONFIG = {\n",
    "    'enable_mixed_precision': True,  # Use mixed precision training if available\n",
    "    'gradient_clipping': 1.0,       # Gradient clipping value\n",
    "    'accumulation_steps': 1,        # Gradient accumulation steps\n",
    "    'memory_efficient': True,       # Enable memory optimizations\n",
    "}\n",
    "\n",
    "def get_config():\n",
    "    \"\"\"Return the complete configuration dictionary\"\"\"\n",
    "    return {\n",
    "        'model': MODEL_CONFIG,\n",
    "        'training': TRAINING_CONFIG,\n",
    "        'augmentation': AUGMENTATION_CONFIG,\n",
    "        'inference': INFERENCE_CONFIG,\n",
    "        'data_paths': DATA_PATHS,\n",
    "        'device': DEVICE_CONFIG,\n",
    "        'video': VIDEO_CONFIG,\n",
    "        'colors': COLORS,\n",
    "        'logging': LOGGING_CONFIG,\n",
    "        'performance': PERFORMANCE_CONFIG,\n",
    "    }\n",
    "\n",
    "def update_config(config_name, key, value):\n",
    "    \"\"\"Update a specific configuration value\"\"\"\n",
    "    config = get_config()\n",
    "    if config_name in config and key in config[config_name]:\n",
    "        config[config_name][key] = value\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def print_config():\n",
    "    \"\"\"Print the current configuration\"\"\"\n",
    "    config = get_config()\n",
    "    print(\"Face Recognition System Configuration:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for section_name, section_config in config.items():\n",
    "        print(f\"\\n{section_name.upper()}:\")\n",
    "        for key, value in section_config.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     print_config() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5833c9d",
   "metadata": {
    "papermill": {
     "duration": 0.018571,
     "end_time": "2025-07-30T13:38:46.656107",
     "exception": false,
     "start_time": "2025-07-30T13:38:46.637536",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Main Files\n",
    "- preprocessing\n",
    "- hyperparameter tuning (`lr` and `wd`)\n",
    "- finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad587a6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T13:38:46.694940Z",
     "iopub.status.busy": "2025-07-30T13:38:46.694724Z",
     "iopub.status.idle": "2025-07-30T13:38:57.641221Z",
     "shell.execute_reply": "2025-07-30T13:38:57.640361Z"
    },
    "papermill": {
     "duration": 10.967588,
     "end_time": "2025-07-30T13:38:57.642521",
     "exception": false,
     "start_time": "2025-07-30T13:38:46.674933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting Face Recognition System Training...\n",
      "Loading dataset...\n",
      "Found 0 artist images\n",
      "Found 0 others images\n",
      "No images found! Please check the data paths.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Face Recognition System - Kaggle Training Script\n",
    "This script is designed to run on Kaggle notebook with P100 GPU (16GB) and 32GB RAM.\n",
    "It fine-tunes an efficient face recognition model to identify a specific artist.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class ArtistDataset(Dataset):\n",
    "    \"\"\"Custom dataset for artist face recognition\"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)['image']\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load and prepare dataset\"\"\"\n",
    "    print(\"Loading dataset...\")\n",
    "    \n",
    "    # Data paths\n",
    "    artist_path = \"/kaggle/input/artist/\"\n",
    "    others_path = \"/kaggle/input/others/\"\n",
    "    \n",
    "    # Collect artist images (label 1)\n",
    "    artist_images = []\n",
    "    if os.path.exists(artist_path):\n",
    "        for filename in os.listdir(artist_path):\n",
    "            if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                artist_images.append(os.path.join(artist_path, filename))\n",
    "    \n",
    "    # Collect others images (label 0)\n",
    "    others_images = []\n",
    "    if os.path.exists(others_path):\n",
    "        for filename in os.listdir(others_path):\n",
    "            if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                others_images.append(os.path.join(others_path, filename))\n",
    "    \n",
    "    print(f\"Found {len(artist_images)} artist images\")\n",
    "    print(f\"Found {len(others_images)} others images\")\n",
    "    \n",
    "    # Create labels\n",
    "    artist_labels = [1] * len(artist_images)\n",
    "    others_labels = [0] * len(others_images)\n",
    "    \n",
    "    # Combine data\n",
    "    all_images = artist_images + others_images\n",
    "    all_labels = artist_labels + others_labels\n",
    "    \n",
    "    return all_images, all_labels\n",
    "\n",
    "def create_transforms():\n",
    "    \"\"\"Create heavy data augmentation transforms\"\"\"\n",
    "    \n",
    "    # Training transforms with heavy augmentation\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(224, 224),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.1),\n",
    "        A.RandomRotate90(p=0.3),\n",
    "        A.Rotate(limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "        A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "        A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
    "        A.GaussianBlur(blur_limit=(3, 7), p=0.2),\n",
    "        A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.2),\n",
    "        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.2),\n",
    "        A.OpticalDistortion(distort_limit=0.2, shift_limit=0.15, p=0.2),\n",
    "        A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.3),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    \n",
    "    # Validation transforms (minimal augmentation)\n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    \n",
    "    return train_transform, val_transform\n",
    "\n",
    "class EfficientFaceModel(nn.Module):\n",
    "    \"\"\"Efficient face recognition model based on MobileNetV2\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=2, dropout_rate=0.5):\n",
    "        super(EfficientFaceModel, self).__init__()\n",
    "        \n",
    "        # Load pre-trained MobileNetV2\n",
    "        self.backbone = models.mobilenet_v2(pretrained=True)\n",
    "        \n",
    "        # Remove the last classifier layer\n",
    "        self.backbone.classifier = nn.Identity()\n",
    "        \n",
    "        # Add custom classifier for binary classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(1280, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone.features(x)\n",
    "        features = features.mean([2, 3])  # Global average pooling\n",
    "        output = self.classifier(features)\n",
    "        return output\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    \"\"\"Validate for one epoch\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(tqdm(dataloader, desc=\"Validation\")):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Optuna objective function for hyperparameter optimization\"\"\"\n",
    "    \n",
    "    # Hyperparameters to optimize\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-2, log=True)\n",
    "    \n",
    "    # Load data\n",
    "    all_images, all_labels = load_data()\n",
    "    \n",
    "    # Split data\n",
    "    train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "        all_images, all_labels, test_size=0.2, random_state=42, stratify=all_labels\n",
    "    )\n",
    "    \n",
    "    # Create transforms\n",
    "    train_transform, val_transform = create_transforms()\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = ArtistDataset(train_images, train_labels, train_transform)\n",
    "    val_dataset = ArtistDataset(val_images, val_labels, val_transform)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = EfficientFaceModel().to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # Training loop (shorter for hyperparameter tuning)\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(10):  # Shorter training for hyperparameter tuning\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            break\n",
    "    \n",
    "    return best_val_loss\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    print(\"Starting Face Recognition System Training...\")\n",
    "    \n",
    "    # Load data\n",
    "    all_images, all_labels = load_data()\n",
    "    \n",
    "    if len(all_images) == 0:\n",
    "        print(\"No images found! Please check the data paths.\")\n",
    "        return\n",
    "    \n",
    "    # Split data\n",
    "    train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "        all_images, all_labels, test_size=0.2, random_state=42, stratify=all_labels\n",
    "    )\n",
    "    \n",
    "    print(f\"Training samples: {len(train_images)}\")\n",
    "    print(f\"Validation samples: {len(val_images)}\")\n",
    "    \n",
    "    # Create transforms\n",
    "    train_transform, val_transform = create_transforms()\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = ArtistDataset(train_images, train_labels, train_transform)\n",
    "    val_dataset = ArtistDataset(val_images, val_labels, val_transform)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "    \n",
    "    # Hyperparameter optimization with Optuna\n",
    "    print(\"Starting hyperparameter optimization...\")\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=10)\n",
    "    \n",
    "    best_params = study.best_params\n",
    "    print(f\"Best hyperparameters: {best_params}\")\n",
    "    \n",
    "    # Train with best hyperparameters\n",
    "    print(\"Training with best hyperparameters...\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = EfficientFaceModel().to(device)\n",
    "    \n",
    "    # Loss and optimizer with best parameters\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=best_params['lr'], \n",
    "        weight_decay=best_params['weight_decay']\n",
    "    )\n",
    "    \n",
    "    # Training history\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 20\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 7\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save(model.state_dict(), 'artist_model_best.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save(model.state_dict(), 'artist_model_final.pth')\n",
    "    print(\"Model saved successfully!\")\n",
    "    \n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accs, label='Train Accuracy')\n",
    "    plt.plot(val_accs, label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Training completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7961995,
     "sourceId": 12605156,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 92.781186,
   "end_time": "2025-07-30T13:38:58.981708",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-30T13:37:26.200522",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

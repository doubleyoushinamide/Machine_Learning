{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12605156,"sourceType":"datasetVersion","datasetId":7961995}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q torch torchvision opencv-python numpy Pillow matplotlib albumentations tqdm  scikit-learn tensorboard","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Config","metadata":{}},{"cell_type":"code","source":"\"\"\"\nConfiguration file for Face Recognition System\nContains all the key parameters for easy tuning and customization.\n\"\"\"\n\n# Model Configuration\nMODEL_CONFIG = {\n    'model_name': 'mobilenet_v2',\n    'num_classes': 2,\n    'input_size': 224,\n    'dropout_rate': 0.5,\n    'pretrained': True\n}\n\n# Training Configuration\nTRAINING_CONFIG = {\n    'batch_size': 16,  # Optimized for 16GB GPU\n    'num_epochs': 20,\n    'early_stopping_patience': 7,\n    'learning_rate_range': (1e-5, 1e-2),  # For Optuna optimization\n    'weight_decay_range': (1e-5, 1e-2),   # For Optuna optimization\n    'optuna_trials': 10,\n    'validation_split': 0.2,\n    'random_seed': 42\n}\n\n# Data Augmentation Configuration\nAUGMENTATION_CONFIG = {\n    'train_transforms': [\n        ('Resize', {'height': 224, 'width': 224}),\n        ('HorizontalFlip', {'p': 0.5}),\n        ('VerticalFlip', {'p': 0.1}),\n        ('RandomRotate90', {'p': 0.3}),\n        ('Rotate', {'limit': 15, 'p': 0.5}),\n        ('RandomBrightnessContrast', {'brightness_limit': 0.2, 'contrast_limit': 0.2, 'p': 0.5}),\n        ('HueSaturationValue', {'hue_shift_limit': 20, 'sat_shift_limit': 30, 'val_shift_limit': 20, 'p': 0.5}),\n        ('GaussNoise', {'var_limit': (10.0, 50.0), 'p': 0.3}),\n        ('GaussianBlur', {'blur_limit': (3, 7), 'p': 0.2}),\n        ('ElasticTransform', {'alpha': 1, 'sigma': 50, 'alpha_affine': 50, 'p': 0.2}),\n        ('GridDistortion', {'num_steps': 5, 'distort_limit': 0.3, 'p': 0.2}),\n        ('OpticalDistortion', {'distort_limit': 0.2, 'shift_limit': 0.15, 'p': 0.2}),\n        ('CoarseDropout', {'max_holes': 8, 'max_height': 32, 'max_width': 32, 'p': 0.3}),\n        ('Normalize', {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}),\n    ],\n    'val_transforms': [\n        ('Resize', {'height': 224, 'width': 224}),\n        ('Normalize', {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}),\n    ]\n}\n\n# Inference Configuration\nINFERENCE_CONFIG = {\n    'confidence_threshold': 0.9,  # Threshold for artist detection\n    'face_detection_confidence': 0.9,  # MTCNN confidence threshold\n    'min_face_size': 20,  # Minimum face size for detection\n    'scale_factor': 0.709,  # MTCNN scale factor\n    'video_frame_skip': 3,  # Process every Nth frame for efficiency\n}\n\n# Data Paths\nDATA_PATHS = {\n    'kaggle_artist_path': '/kaggle/input/input-data/Davido',\n    'kaggle_others_path': '/kaggle/input/input-data/Unknown',\n    'model_save_path': 'artist_model_best.pth',\n    'final_model_path': 'artist_model_final.pth',\n    'training_curves_path': 'training_curves.png'\n}\n\n# Device Configuration\nDEVICE_CONFIG = {\n    'use_gpu': True,\n    'num_workers': 2,  # For data loading\n    'pin_memory': True,\n}\n\n# Video Processing Configuration\nVIDEO_CONFIG = {\n    'output_fps': 30,\n    'output_codec': 'mp4v',\n    'draw_boxes': True,\n    'show_probabilities': True,\n    'box_thickness': 2,\n    'text_scale': 0.5,\n    'text_thickness': 2,\n}\n\n# Colors for visualization\nCOLORS = {\n    'artist_face': (0, 255, 0),    # Green for artist\n    'other_face': (0, 0, 255),     # Red for others\n    'text_color': (255, 255, 255), # White text\n}\n\n# Logging Configuration\nLOGGING_CONFIG = {\n    'log_level': 'INFO',\n    'log_format': '%(asctime)s - %(levelname)s - %(message)s',\n    'save_logs': True,\n    'log_file': 'face_recognition.log'\n}\n\n# Performance Configuration\nPERFORMANCE_CONFIG = {\n    'enable_mixed_precision': True,  # Use mixed precision training if available\n    'gradient_clipping': 1.0,       # Gradient clipping value\n    'accumulation_steps': 1,        # Gradient accumulation steps\n    'memory_efficient': True,       # Enable memory optimizations\n}\n\ndef get_config():\n    \"\"\"Return the complete configuration dictionary\"\"\"\n    return {\n        'model': MODEL_CONFIG,\n        'training': TRAINING_CONFIG,\n        'augmentation': AUGMENTATION_CONFIG,\n        'inference': INFERENCE_CONFIG,\n        'data_paths': DATA_PATHS,\n        'device': DEVICE_CONFIG,\n        'video': VIDEO_CONFIG,\n        'colors': COLORS,\n        'logging': LOGGING_CONFIG,\n        'performance': PERFORMANCE_CONFIG,\n    }\n\ndef update_config(config_name, key, value):\n    \"\"\"Update a specific configuration value\"\"\"\n    config = get_config()\n    if config_name in config and key in config[config_name]:\n        config[config_name][key] = value\n        return True\n    return False\n\ndef print_config():\n    \"\"\"Print the current configuration\"\"\"\n    config = get_config()\n    print(\"Face Recognition System Configuration:\")\n    print(\"=\" * 50)\n    \n    for section_name, section_config in config.items():\n        print(f\"\\n{section_name.upper()}:\")\n        for key, value in section_config.items():\n            print(f\"  {key}: {value}\")\n\n# if __name__ == \"__main__\":\n#     print_config() ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Main Files\n- preprocessing\n- hyperparameter tuning (`lr` and `wd`)\n- finetuning","metadata":{}},{"cell_type":"code","source":"\"\"\"\nFace Recognition System - Kaggle Training Script\nThis script is designed to run on Kaggle notebook with P100 GPU (16GB) and 32GB RAM.\nIt fine-tunes an efficient face recognition model to identify a specific artist.\n\"\"\"\n\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms, models\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport optuna\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\nclass ArtistDataset(Dataset):\n    \"\"\"Custom dataset for artist face recognition\"\"\"\n    \n    def __init__(self, image_paths, labels, transform=None):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        label = self.labels[idx]\n        \n        # Load image\n        image = Image.open(image_path).convert('RGB')\n        image = np.array(image)\n        \n        if self.transform:\n            image = self.transform(image=image)['image']\n        \n        return image, label\n\ndef load_data():\n    \"\"\"Load and prepare dataset\"\"\"\n    print(\"Loading dataset...\")\n    \n    # Data paths\n    artist_path = \"/kaggle/input/artist/\"\n    others_path = \"/kaggle/input/others/\"\n    \n    # Collect artist images (label 1)\n    artist_images = []\n    if os.path.exists(artist_path):\n        for filename in os.listdir(artist_path):\n            if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n                artist_images.append(os.path.join(artist_path, filename))\n    \n    # Collect others images (label 0)\n    others_images = []\n    if os.path.exists(others_path):\n        for filename in os.listdir(others_path):\n            if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n                others_images.append(os.path.join(others_path, filename))\n    \n    print(f\"Found {len(artist_images)} artist images\")\n    print(f\"Found {len(others_images)} others images\")\n    \n    # Create labels\n    artist_labels = [1] * len(artist_images)\n    others_labels = [0] * len(others_images)\n    \n    # Combine data\n    all_images = artist_images + others_images\n    all_labels = artist_labels + others_labels\n    \n    return all_images, all_labels\n\ndef create_transforms():\n    \"\"\"Create heavy data augmentation transforms\"\"\"\n    \n    # Training transforms with heavy augmentation\n    train_transform = A.Compose([\n        A.Resize(224, 224),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.1),\n        A.RandomRotate90(p=0.3),\n        A.Rotate(limit=15, p=0.5),\n        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n        A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n        A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n        A.GaussianBlur(blur_limit=(3, 7), p=0.2),\n        A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.2),\n        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.2),\n        A.OpticalDistortion(distort_limit=0.2, shift_limit=0.15, p=0.2),\n        A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.3),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2(),\n    ])\n    \n    # Validation transforms (minimal augmentation)\n    val_transform = A.Compose([\n        A.Resize(224, 224),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2(),\n    ])\n    \n    return train_transform, val_transform\n\nclass EfficientFaceModel(nn.Module):\n    \"\"\"Efficient face recognition model based on MobileNetV2\"\"\"\n    \n    def __init__(self, num_classes=2, dropout_rate=0.5):\n        super(EfficientFaceModel, self).__init__()\n        \n        # Load pre-trained MobileNetV2\n        self.backbone = models.mobilenet_v2(pretrained=True)\n        \n        # Remove the last classifier layer\n        self.backbone.classifier = nn.Identity()\n        \n        # Add custom classifier for binary classification\n        self.classifier = nn.Sequential(\n            nn.Dropout(dropout_rate),\n            nn.Linear(1280, 512),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(512, 128),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(128, num_classes)\n        )\n    \n    def forward(self, x):\n        features = self.backbone.features(x)\n        features = features.mean([2, 3])  # Global average pooling\n        output = self.classifier(features)\n        return output\n\ndef train_epoch(model, dataloader, criterion, optimizer, device):\n    \"\"\"Train for one epoch\"\"\"\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for batch_idx, (data, target) in enumerate(tqdm(dataloader, desc=\"Training\")):\n        data, target = data.to(device), target.to(device)\n        \n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        _, predicted = output.max(1)\n        total += target.size(0)\n        correct += predicted.eq(target).sum().item()\n    \n    epoch_loss = running_loss / len(dataloader)\n    epoch_acc = 100. * correct / total\n    return epoch_loss, epoch_acc\n\ndef validate_epoch(model, dataloader, criterion, device):\n    \"\"\"Validate for one epoch\"\"\"\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for batch_idx, (data, target) in enumerate(tqdm(dataloader, desc=\"Validation\")):\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            loss = criterion(output, target)\n            \n            running_loss += loss.item()\n            _, predicted = output.max(1)\n            total += target.size(0)\n            correct += predicted.eq(target).sum().item()\n    \n    epoch_loss = running_loss / len(dataloader)\n    epoch_acc = 100. * correct / total\n    return epoch_loss, epoch_acc\n\ndef objective(trial):\n    \"\"\"Optuna objective function for hyperparameter optimization\"\"\"\n    \n    # Hyperparameters to optimize\n    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-2, log=True)\n    \n    # Load data\n    all_images, all_labels = load_data()\n    \n    # Split data\n    train_images, val_images, train_labels, val_labels = train_test_split(\n        all_images, all_labels, test_size=0.2, random_state=42, stratify=all_labels\n    )\n    \n    # Create transforms\n    train_transform, val_transform = create_transforms()\n    \n    # Create datasets\n    train_dataset = ArtistDataset(train_images, train_labels, train_transform)\n    val_dataset = ArtistDataset(val_images, val_labels, val_transform)\n    \n    # Create dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n    \n    # Initialize model\n    model = EfficientFaceModel().to(device)\n    \n    # Loss and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    \n    # Training loop (shorter for hyperparameter tuning)\n    best_val_loss = float('inf')\n    patience = 5\n    patience_counter = 0\n    \n    for epoch in range(10):  # Shorter training for hyperparameter tuning\n        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n        \n        # Early stopping\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            \n        if patience_counter >= patience:\n            break\n    \n    return best_val_loss\n\ndef main():\n    \"\"\"Main training function\"\"\"\n    print(\"Starting Face Recognition System Training...\")\n    \n    # Load data\n    all_images, all_labels = load_data()\n    \n    if len(all_images) == 0:\n        print(\"No images found! Please check the data paths.\")\n        return\n    \n    # Split data\n    train_images, val_images, train_labels, val_labels = train_test_split(\n        all_images, all_labels, test_size=0.2, random_state=42, stratify=all_labels\n    )\n    \n    print(f\"Training samples: {len(train_images)}\")\n    print(f\"Validation samples: {len(val_images)}\")\n    \n    # Create transforms\n    train_transform, val_transform = create_transforms()\n    \n    # Create datasets\n    train_dataset = ArtistDataset(train_images, train_labels, train_transform)\n    val_dataset = ArtistDataset(val_images, val_labels, val_transform)\n    \n    # Create dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n    \n    # Hyperparameter optimization with Optuna\n    print(\"Starting hyperparameter optimization...\")\n    study = optuna.create_study(direction='minimize')\n    study.optimize(objective, n_trials=10)\n    \n    best_params = study.best_params\n    print(f\"Best hyperparameters: {best_params}\")\n    \n    # Train with best hyperparameters\n    print(\"Training with best hyperparameters...\")\n    \n    # Initialize model\n    model = EfficientFaceModel().to(device)\n    \n    # Loss and optimizer with best parameters\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(\n        model.parameters(), \n        lr=best_params['lr'], \n        weight_decay=best_params['weight_decay']\n    )\n    \n    # Training history\n    train_losses = []\n    val_losses = []\n    train_accs = []\n    val_accs = []\n    \n    # Training loop\n    num_epochs = 20\n    best_val_loss = float('inf')\n    patience = 7\n    patience_counter = 0\n    \n    for epoch in range(num_epochs):\n        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n        \n        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n        \n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        train_accs.append(train_acc)\n        val_accs.append(val_acc)\n        \n        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n        \n        # Early stopping\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            # Save best model\n            torch.save(model.state_dict(), 'artist_model_best.pth')\n        else:\n            patience_counter += 1\n            \n        if patience_counter >= patience:\n            print(\"Early stopping triggered!\")\n            break\n    \n    # Save final model\n    torch.save(model.state_dict(), 'artist_model_final.pth')\n    print(\"Model saved successfully!\")\n    \n    # Plot training curves\n    plt.figure(figsize=(12, 4))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(train_losses, label='Train Loss')\n    plt.plot(val_losses, label='Validation Loss')\n    plt.title('Training and Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(train_accs, label='Train Accuracy')\n    plt.plot(val_accs, label='Validation Accuracy')\n    plt.title('Training and Validation Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy (%)')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.savefig('training_curves.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    print(\"Training completed successfully!\")\n\nif __name__ == \"__main__\":\n    main() ","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}